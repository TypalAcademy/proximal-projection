{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Proximal Projection Algorithm","text":"<p>This website documents code used in the paper Proximal Projection Method for Stable Linearly Constrained Optimization.</p> <p> <p>arXiv Prerint  Github Repo </p> <p></p>"},{"location":"#abstract","title":"Abstract","text":"<p>Many applications using large datasets require efficient methods for minimizing a proximable convex function subject to satisfying a set of linear constraints within a specified tolerance. For this task, we present a proximal projection (PP) algorithm, which is an instance of Douglas-Rachford splitting that directly uses projections onto the set of constraints. Formal guarantees are presented to prove convergence of PP estimates to optimizers. Unlike many methods that obtain feasibility asymptotically, each PP iterate is feasible. Numerically, we show PP either matches or outperforms alternatives (e.g. linearized Bregman, primal dual hybrid gradient, proximal augmented Lagrangian, proximal gradient) on problems in basis pursuit, stable matrix completion, stable principal component pursuit, and the computation of earth mover's distances.</p>"},{"location":"#key-results","title":"Key Results","text":"<p>Problem: For a convex function \\(\\mathsf{f\\colon\\mathbb{R}^n \\rightarrow \\overline{\\mathbb{R}}}\\), a matrix \\(\\mathsf{A \\in \\mathbb{R}^{m\\times n}}\\), a vector \\(\\mathsf{b\\in \\mathbb{R}^m}\\), and a scalar \\(\\mathsf{\\varepsilon \\geq 0}\\), we consider the problem</p> \\[ \\mathsf{   \\underset{x}{\\mathsf{min}} \\ f(x) \\quad\\mathsf{s.t.}\\quad \\|Ax-b\\|\\leq \\varepsilon. } \\] <p>We refer to this as a stable linearly constrained optimization problem and below set \\(\\mathsf{\\mathcal{C} = \\{ x : \\|Ax-b\\|\\leq\\varepsilon\\}}\\). The key conditions used in this work are as follows.</p> <ul> <li> the function \\(\\mathsf{f\\colon\\mathbb{R}^n\\rightarrow \\overline{\\mathbb{R}}}\\) is closed, convex, and proper;</li> <li> either the matrix \\(\\mathsf{A}\\) has full row-rank or \\(\\mathsf{\\varepsilon &gt; 0}\\);</li> <li> there is \\(\\mathsf{y \\in \\mathbb{R}^n}\\) such that, if \\(\\mathsf{\\varepsilon = 0}\\), then \\(\\mathsf{Ay = b}\\) and, if \\(\\mathsf{\\varepsilon &gt; 0}\\), then  \\(\\mathsf{\\|Ay-b\\| &lt; \\varepsilon}\\); </li> <li> the above condition holds for \\(\\mathsf{y \\in \\mbox{ri}(\\mbox{dom}(f))}\\).    </li> <li> either \\(\\mathsf{f}\\) is coercive or \\(\\mathcal{C}\\) is bounded; </li> </ul> <p>Proposition: If the conditions above hold, then projection onto \\(\\mathcal{C}\\) is given by</p> \\[ \\mathsf{proj_{\\mathcal{C}}(x)} = \\begin{cases}     \\begin{array}{cl}         \\mathsf{x} &amp; \\mathsf{if\\ \\|Ax-b\\|\\leq\\varepsilon,} \\\\         \\mathsf{x - A^\\top(AA^\\top +\\varepsilon\\tau_x I)^{-1}(Ax-b)} &amp; \\mathsf{otherwise},     \\end{array} \\end{cases} \\] <p>where, if \\(\\mathsf{\\|Ax-b\\|&gt;\\varepsilon}\\), the scalar \\(\\mathsf{\\tau_x}\\) is the unique positive solution to </p> \\[   \\mathsf{1 = \\tau \\| (AA^\\top +\\varepsilon \\tau I)^{-1} (Ax-b) \\|.} \\] <p>Using this projection with Douglas-Rachford Splitting (DRS) yields the PP algorithm below. The output \\(\\mathsf{x}\\) estimates a minimizer to our problem.</p> \\[   \\begin{align}     &amp; \\mathsf{while\\ \\ stopping\\ \\  criteria\\ \\ not\\ \\ met} \\\\     &amp; \\quad\\quad \\mathsf{if \\ \\|Az-b\\|\\leq \\varepsilon} \\\\     &amp; \\quad\\quad\\quad\\quad \\mathsf{x \\leftarrow z} \\\\     &amp; \\quad\\quad \\mathsf{else} \\\\     &amp; \\quad\\quad\\quad\\quad \\mathsf{\\tau \\leftarrow solution\\big( 1 = \\tau \\|AA^\\top + \\varepsilon I)^{-1}(Az-b)} \\\\     &amp; \\quad\\quad\\quad\\quad \\mathsf{x \\leftarrow z - A^\\top (AA^\\top+\\varepsilon I)^{-1}(Az-b) } \\\\     &amp; \\quad\\quad \\mathsf{z \\leftarrow z + prox_{\\alpha f}(2x - z) - x}\\\\     &amp; \\mathsf{return\\ \\ x}   \\end{align} \\] <p>Theorem: If the listed conditions hold, then PP converges to a solution of the stable linearly constrained optimization problem.</p>"},{"location":"#citation","title":"Citation","text":"<pre><code>@article{heaton2024proximal,\n         title={{Proximal Projection Method for Stable Linearly Constrained Optimization}},\n         author={Heaton, Howard},\n         journal={{arXiv preprint}},\n         year={2024}\n}\n</code></pre> <p> Contact Us  </p> <p></p>"},{"location":"basis_pursuit/","title":"Basis Pursuit","text":"<p>This page shows the numerical example for solving the problem</p> \\[     \\mathsf{ \\underset{x}{min} \\ |x|_1 \\quad s.t. \\quad Ax=b. } \\] <p>The proximal for \\(|\\)x\\(|_1\\) is an element-wise shrink operation.</p> Source code in <code>examples/basis_pursuit/methods.py</code> <pre><code>def shrink(xi: NDArray[np.float64], alpha: float) -&gt; NDArray[np.float64]:\n    \"\"\"The proximal for $|$x$|_1$ is an element-wise shrink operation.\"\"\"\n    output: NDArray[np.float64] = np.sign(xi) * np.maximum(np.abs(xi) - alpha, 0)\n    return output\n</code></pre>"},{"location":"basis_pursuit/#methods","title":"Methods","text":"<p>The update iteration for each benchmarked algorithm is in Appendix B.1 of the paper. Code implementations are shown below.</p> <p>Proximal Projection</p> Source code in <code>examples/basis_pursuit/methods.py</code> <pre><code>def proximal_projection(A, b, alpha=1.0e-1, num_iters=2000):\n    \"\"\"Proximal Projection\"\"\"\n    stats = OptimizationStats(matrix=A, measurements=b)\n    start = time.time()\n    z = np.zeros((A.shape[1], 1))\n    x = np.zeros((A.shape[1], 1))\n    AAt = np.linalg.inv(A @ A.T)\n    for _ in range(num_iters):\n        x_p = x.copy()\n        x = z - A.T @ (AAt @ (A @ z - b))\n        z = z + shrink(2.0 * x - z, alpha) - x\n        stats.add_iteration(x, x_p)\n    stats.execution_time = time.time() - start\n    return x, stats\n</code></pre> <p>Linearized Bregman</p> Source code in <code>examples/basis_pursuit/methods.py</code> <pre><code>def linearized_bregman(A, b, mu=2.0, num_iters=2000):\n    \"\"\"Linearized Bregman\"\"\"\n    stats = OptimizationStats(matrix=A, measurements=b)\n    start = time.time()\n    x = np.zeros((A.shape[1], 1))\n    v = np.zeros((A.shape[1], 1))\n    matrix_norm = np.linalg.norm(A @ A.T)\n    alpha = 2.0 / matrix_norm\n    mu *= matrix_norm\n    for _ in range(num_iters):\n        x_p = x.copy()\n        v = v - A.T @ (A @ x - b)\n        x = shrink(alpha * v, alpha * mu)\n        stats.add_iteration(x, x_p)\n    stats.execution_time = time.time() - start\n    return x, stats\n</code></pre> <p>Linearized Method of Multipliers</p> Source code in <code>examples/basis_pursuit/methods.py</code> <pre><code>def linearized_method_multipliers(A, b, lambd=100.0, num_iters=2000):\n    \"\"\"Linearized Method of Multipliers\"\"\"\n    stats = OptimizationStats(matrix=A, measurements=b)\n    start = time.time()\n    rows, cols = A.shape\n    x = np.zeros((cols, 1))\n    v = np.zeros((rows, 1))\n    lambd *= np.linalg.norm(A.T @ A)\n    alpha = 1.0 / (lambd * np.linalg.norm(A.T @ A))\n    for _ in range(num_iters):\n        x_p = x.copy()\n        x = shrink(x - alpha * A.T @ (v + lambd * (A @ x - b)), alpha)\n        v = v + lambd * (A @ x - b)\n        stats.add_iteration(x, x_p)\n    stats.execution_time = time.time() - start\n    return x, stats\n</code></pre> <p>Primal Dual Hybrid Gradient</p> Source code in <code>examples/basis_pursuit/methods.py</code> <pre><code>def primal_dual_hybrid_gradient(A, b, lambd=100.0, num_iters=2000):\n    \"\"\"Primal Dual Hybrid Gradient\"\"\"\n    stats = OptimizationStats(matrix=A, measurements=b)\n    start = time.time()\n    rows, cols = A.shape\n    x = np.zeros((cols, 1))\n    v = np.zeros((rows, 1))\n    alpha = 1.0 / (lambd * np.linalg.norm(A.T @ A))\n    for _ in range(num_iters):\n        x_p = x.copy()\n        x = shrink(x - alpha * A.T @ v, alpha)\n        v = v + lambd * (A @ (2 * x - x_p) - b)\n        stats.add_iteration(x, x_p)\n    stats.execution_time = time.time() - start\n    return x, stats\n</code></pre> <p></p>"},{"location":"basis_pursuit/#experiment","title":"Experiment","text":"<p>The matrix \\(\\mathsf{A \\in \\mathbb{R}^{m\\times n}}\\) i.i.d. Gaussian entries, with \\(\\mathsf{m=500}\\) and \\(\\mathsf{n=2000}\\). Elements of a sparse vector \\(\\mathsf{x^\\star}\\) are independently nonzero with probability \\(\\mathsf{p = 0.05}\\) and the nonzero values are i.i.d. Gaussian. This is used to compute \\(\\mathsf{b = Ax^\\star}\\). Ten trials are executed with distinct random seeds. The mean time for 10 trials of proximal projection, linearized Bregman, primal dual hybrid gradient, and linearized method of multipliers to compute 2,000 iterations were, respectively, 8.06s, 7.40s, 20.60s, and 22.85s.</p> <p>Execute numerical experiment for basis pursuit</p> Source code in <code>examples/basis_pursuit/basis_pursuit_experiment.py</code> <pre><code>def run_basis_pursuit_experiment(seeds=1, iters=2000, m=500, n=2000) -&gt; None:\n    \"\"\"Execute numerical experiment for basis pursuit\"\"\"\n    methods = {\"pp\": proximal_projection, \n               \"lmm\": linearized_method_multipliers,\n               \"lb\": linearized_bregman,\n               \"pdhg\": primal_dual_hybrid_gradient}\n    metrics = [\"obj\", \"res\", \"viol\", \"time\"]\n\n    exp_stats = {name: {} for name in methods.keys()}\n    for method in exp_stats.keys():\n        for metric in metrics:\n            exp_stats[method][metric] = np.zeros((seeds, 1)) if metric == \"time\" else np.zeros((seeds, iters))\n\n    print('Beginning numerical experiment')\n    for seed in range(seeds):\n        print(f\"seed = {(seed+1):4} of {seeds}\")\n        np.random.seed(seed)\n        A = np.random.normal(0, 1.0 / m, size=(m, n))\n        x = np.random.normal(0, 1.0, size=(n, 1)) * np.random.binomial(n=1, p=0.05, size=(n, 1))\n        b = A @ x\n\n        for method_name, method in methods.items():\n            stats = method(A, b, num_iters=iters)[1]\n            for metric in metrics:\n                exp_stats[method_name][metric][seed, :] = stats.metrics()[metric]\n\n    print('Computing summary statistics')\n    summary = {name: {} for name in methods.keys()}\n    for method_name, method in methods.items():\n        for metric in metrics:\n            summary[method_name][metric] = np.median(exp_stats[method_name][metric], axis=0)\n\n    # print('Making plots')\n    # fig, ax = plt.subplots()\n    # plt.title(\"Violation $|Ax-b|$\")\n    # plt.plot(viol_pp, color=\"b\")\n    # plt.plot(viol_lb, color=\"g\")\n    # plt.plot(viol_lmm, color=\"k\")\n    # plt.plot(viol_pdhg, color=\"r\")\n    # plt.yscale(\"log\")\n    # plt.show()\n\n    # fig, ax = plt.subplots()\n    # plt.title(\"Objective |x^k|_1\")\n    # plt.plot(obj_pp, color=\"b\")\n    # plt.plot(obj_lb, color=\"g\")\n    # plt.plot(obj_lmm, color=\"k\")\n    # plt.plot(obj_pdhg, color=\"r\")\n    # plt.show()\n\n    # fig, ax = plt.subplots()\n    # plt.title(\"Residual $|x^{k+1}-x^k|$\")\n    # plt.plot(res_pp, color=\"b\")\n    # plt.plot(res_lb, color=\"g\")\n    # plt.plot(res_lmm, color=\"k\")\n    # plt.plot(res_pdhg, color=\"r\")\n    # plt.yscale(\"log\")\n    # plt.show()    \n\n    print('Writing summary statistics to file')\n    for metric in [\"obj\", \"res\", \"viol\"]:\n        filename = f\"bp-{metric}-plots.csv\"\n        with open(filename, \"w\") as csv_file:\n            for k in range(iters): \n                vals = [f'{float(summary[name][metric][k]):0.5e}' for name in methods]\n                msg = \",\".join(vals) + \"\\n\"\n                csv_file.write(msg)        \n\n    with open(\"bp-times.tex\", \"w\") as csv_file:\n        csv_file.write(f\"\\\\def\\\\bpTimePP{{{float(summary[\"pp\"][\"time\"]):0.2f}}}\\n\")\n        csv_file.write(f\"\\\\def\\\\bpTimeLB{{{float(summary[\"lb\"][\"time\"]):0.2f}}}\\n\")\n        csv_file.write(f\"\\\\def\\\\bpTimePDHG{{{float(summary[\"pdhg\"][\"time\"]):0.2f}}}\\n\")\n        csv_file.write(f\"\\\\def\\\\bpTimeLMM{{{float(summary[\"lmm\"][\"time\"]):0.2f}}}\")           \n</code></pre> <p></p>"},{"location":"proximal_projection/","title":"Proximal Projection","text":"<p>This page shows the numerical example for solving the problem</p> \\[     \\mathsf{ \\underset{x}{min} \\ |x|_1 \\quad s.t. \\quad |Ax-b|\\leq \\varepsilon. } \\] <p></p>"}]}